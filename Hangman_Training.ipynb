{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5292251e-4d0d-47b9-b66f-16abdabc08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a66eb3c-60bf-464b-b635-64674f124a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIG ====\n",
    "TXT_PATH = \"words_250000_train.txt\"  # path to provided word list\n",
    "SEED = 42\n",
    "TRAIN_FRACTION = 0.975\n",
    "\n",
    "# Model dims (match what you'll load in guess())\n",
    "VOCAB_SIZE = 29\n",
    "DMODEL = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 4\n",
    "D_FF = 512\n",
    "MAX_LEN = 32  # >= 1 + max word length for Hangman [CLS] input\n",
    "\n",
    "# Pretrain\n",
    "PRE_EPOCHS = 100\n",
    "PRE_BATCH  = 2048\n",
    "PRE_LR     = 1e-3\n",
    "\n",
    "# Finetune (Hangman)\n",
    "FT_EPOCHS  = 100\n",
    "FT_BATCH   = 2048\n",
    "FT_LR      = 3e-4\n",
    "PERMS_PER_WORD = 4  # <- number of random unique-letter permutations per word for data aug\n",
    "\n",
    "# Device\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ==== IMPORTS ====\n",
    "import re, random, string, math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ==== TOKENIZER ====\n",
    "class CharTokenizer:\n",
    "    \"\"\"\n",
    "    Vocab ids:\n",
    "      0 [PAD]  (unused)\n",
    "      1 [MASK] (Hangman inputs only)\n",
    "      2 [CLS]  (Hangman inputs only)\n",
    "      3..28 'a'..'z'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pad_id  = 0\n",
    "        self.mask_id = 1\n",
    "        self.cls_id  = 2\n",
    "        self.letters = [chr(i) for i in range(97,123)]\n",
    "        self.letter2id = {ch: 3 + (ord(ch)-97) for ch in self.letters}\n",
    "        self.vocab_size = 29\n",
    "\n",
    "    def encode_board(self, pattern: str):\n",
    "        # pattern e.g. \"_pp_e\" → [MASK, 'p','p',MASK,'e']\n",
    "        return [self.mask_id if ch == \"_\" else self.letter2id[ch] for ch in pattern]\n",
    "\n",
    "    def with_cls(self, ids):  # prepend [CLS]\n",
    "        return [self.cls_id] + ids\n",
    "\n",
    "tok = CharTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336863f8-b19e-4680-b94d-1f3c20b15631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#words: 227300 | min_len=1 | max_len=29\n",
      "train buckets (top 10 by count): {9: 30136, 8: 29686, 10: 26291, 7: 25243, 11: 22201, 6: 19079, 12: 17734, 13: 12645, 5: 10993, 14: 8486}\n"
     ]
    }
   ],
   "source": [
    "def load_words(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ws = [w.strip().lower() for w in f]\n",
    "    return [w for w in ws if re.fullmatch(r\"[a-z]+\", w)]\n",
    "\n",
    "all_words = load_words(TXT_PATH)\n",
    "print(f\"#words: {len(all_words)} | min_len={min(map(len,all_words))} | max_len={max(map(len,all_words))}\")\n",
    "\n",
    "# Split by word\n",
    "idx_all = list(range(len(all_words)))\n",
    "random.shuffle(idx_all)\n",
    "cut = int(TRAIN_FRACTION * len(idx_all))\n",
    "train_idx = set(idx_all[:cut])\n",
    "val_idx   = set(idx_all[cut:])\n",
    "\n",
    "def bucket_by_len(index_set):\n",
    "    by_len = defaultdict(list)\n",
    "    for i in index_set:\n",
    "        by_len[len(all_words[i])].append(i)\n",
    "    return by_len\n",
    "\n",
    "train_by_len = bucket_by_len(train_idx)\n",
    "val_by_len   = bucket_by_len(val_idx)\n",
    "\n",
    "print(\"train buckets (top 10 by count):\",\n",
    "      dict(list(sorted(((L,len(v)) for L,v in train_by_len.items()), key=lambda x:-x[1]))[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac4461f-5109-4fb4-92db-1b4068984316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyCharTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, d_model=DMODEL, n_heads=N_HEADS,\n",
    "                 n_layers=N_LAYERS, d_ff=D_FF, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb   = nn.Embedding(max_len, d_model)\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads,\n",
    "                                         dim_feedforward=d_ff, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "        self.cls_head = nn.Linear(d_model, 26)  # used per-position in pretrain; [CLS] in finetune\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def _encode(self, token_ids, attn_mask=None):\n",
    "        # token_ids [B,T]\n",
    "        B, T = token_ids.size()\n",
    "        if T > self.max_len:\n",
    "            raise ValueError(f\"seq len {T} > max_len {self.max_len}\")\n",
    "        pos = torch.arange(T, device=token_ids.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_emb(token_ids) + self.pos_emb(pos)\n",
    "        h = self.encoder(x, mask=attn_mask)  # [B,T,d]\n",
    "        return h\n",
    "\n",
    "    # Pretrain: project every position\n",
    "    def forward_per_pos(self, token_ids, attn_mask):\n",
    "        h = self._encode(token_ids, attn_mask)\n",
    "        return self.cls_head(h)  # [B,T,26]\n",
    "\n",
    "    # Finetune: project [CLS] pooled state\n",
    "    def forward_cls(self, token_ids):\n",
    "        h = self._encode(token_ids, attn_mask=None)\n",
    "        cls = h[:,0,:]\n",
    "        return self.cls_head(cls)  # [B,26]\n",
    "\n",
    "def init_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "model = TinyCharTransformer().to(DEVICE)\n",
    "#init_weights(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87dfe27c-23ca-452d-a3a8-7db10db7fc20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BiDir Next-Char Pretrain 1/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 1/100 — VAL   ===\n",
      "[NXT 1/100] loss/tok 2.6154/2.6078 | ppl 13.67/13.57\n",
      "\n",
      "=== BiDir Next-Char Pretrain 2/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 2/100 — VAL   ===\n",
      "[NXT 2/100] loss/tok 2.4709/2.4314 | ppl 11.83/11.37\n",
      "\n",
      "=== BiDir Next-Char Pretrain 3/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 3/100 — VAL   ===\n",
      "[NXT 3/100] loss/tok 2.4092/2.3256 | ppl 11.12/10.23\n",
      "\n",
      "=== BiDir Next-Char Pretrain 4/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 4/100 — VAL   ===\n",
      "[NXT 4/100] loss/tok 2.3402/2.3724 | ppl 10.38/10.72\n",
      "\n",
      "=== BiDir Next-Char Pretrain 5/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 5/100 — VAL   ===\n",
      "[NXT 5/100] loss/tok 2.3038/2.3637 | ppl 10.01/10.63\n",
      "\n",
      "=== BiDir Next-Char Pretrain 6/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 6/100 — VAL   ===\n",
      "[NXT 6/100] loss/tok 2.3074/2.2452 | ppl 10.05/9.44\n",
      "\n",
      "=== BiDir Next-Char Pretrain 7/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 7/100 — VAL   ===\n",
      "[NXT 7/100] loss/tok 2.2694/2.2018 | ppl 9.67/9.04\n",
      "\n",
      "=== BiDir Next-Char Pretrain 8/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 8/100 — VAL   ===\n",
      "[NXT 8/100] loss/tok 2.2369/2.2349 | ppl 9.36/9.35\n",
      "\n",
      "=== BiDir Next-Char Pretrain 9/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 9/100 — VAL   ===\n",
      "[NXT 9/100] loss/tok 2.2428/2.1986 | ppl 9.42/9.01\n",
      "\n",
      "=== BiDir Next-Char Pretrain 10/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 10/100 — VAL   ===\n",
      "[NXT 10/100] loss/tok 2.2309/2.2065 | ppl 9.31/9.08\n",
      "\n",
      "=== BiDir Next-Char Pretrain 11/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 11/100 — VAL   ===\n",
      "[NXT 11/100] loss/tok 2.1948/2.2249 | ppl 8.98/9.25\n",
      "\n",
      "=== BiDir Next-Char Pretrain 12/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 12/100 — VAL   ===\n",
      "[NXT 12/100] loss/tok 2.2087/2.1383 | ppl 9.10/8.49\n",
      "\n",
      "=== BiDir Next-Char Pretrain 13/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 13/100 — VAL   ===\n",
      "[NXT 13/100] loss/tok 2.1912/2.1568 | ppl 8.95/8.64\n",
      "\n",
      "=== BiDir Next-Char Pretrain 14/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 14/100 — VAL   ===\n",
      "[NXT 14/100] loss/tok 2.1737/2.1688 | ppl 8.79/8.75\n",
      "\n",
      "=== BiDir Next-Char Pretrain 15/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 15/100 — VAL   ===\n",
      "[NXT 15/100] loss/tok 2.1926/2.1102 | ppl 8.96/8.25\n",
      "\n",
      "=== BiDir Next-Char Pretrain 16/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 16/100 — VAL   ===\n",
      "[NXT 16/100] loss/tok 2.1731/2.1219 | ppl 8.79/8.35\n",
      "\n",
      "=== BiDir Next-Char Pretrain 17/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 17/100 — VAL   ===\n",
      "[NXT 17/100] loss/tok 2.1614/2.1170 | ppl 8.68/8.31\n",
      "\n",
      "=== BiDir Next-Char Pretrain 18/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 18/100 — VAL   ===\n",
      "[NXT 18/100] loss/tok 2.1557/2.0916 | ppl 8.63/8.10\n",
      "\n",
      "=== BiDir Next-Char Pretrain 19/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 19/100 — VAL   ===\n",
      "[NXT 19/100] loss/tok 2.1549/2.0917 | ppl 8.63/8.10\n",
      "\n",
      "=== BiDir Next-Char Pretrain 20/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 20/100 — VAL   ===\n",
      "[NXT 20/100] loss/tok 2.1339/2.0805 | ppl 8.45/8.01\n",
      "\n",
      "=== BiDir Next-Char Pretrain 21/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 21/100 — VAL   ===\n",
      "[NXT 21/100] loss/tok 2.1283/2.0832 | ppl 8.40/8.03\n",
      "\n",
      "=== BiDir Next-Char Pretrain 22/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 22/100 — VAL   ===\n",
      "[NXT 22/100] loss/tok 2.1226/2.1079 | ppl 8.35/8.23\n",
      "\n",
      "=== BiDir Next-Char Pretrain 23/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 23/100 — VAL   ===\n",
      "[NXT 23/100] loss/tok 2.1139/2.0733 | ppl 8.28/7.95\n",
      "\n",
      "=== BiDir Next-Char Pretrain 24/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 24/100 — VAL   ===\n",
      "[NXT 24/100] loss/tok 2.1321/2.0618 | ppl 8.43/7.86\n",
      "\n",
      "=== BiDir Next-Char Pretrain 25/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 25/100 — VAL   ===\n",
      "[NXT 25/100] loss/tok 2.1204/2.0625 | ppl 8.33/7.87\n",
      "\n",
      "=== BiDir Next-Char Pretrain 26/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 26/100 — VAL   ===\n",
      "[NXT 26/100] loss/tok 2.1199/2.0579 | ppl 8.33/7.83\n",
      "\n",
      "=== BiDir Next-Char Pretrain 27/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 27/100 — VAL   ===\n",
      "[NXT 27/100] loss/tok 2.1007/2.0856 | ppl 8.17/8.05\n",
      "\n",
      "=== BiDir Next-Char Pretrain 28/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 28/100 — VAL   ===\n",
      "[NXT 28/100] loss/tok 2.0981/2.0467 | ppl 8.15/7.74\n",
      "\n",
      "=== BiDir Next-Char Pretrain 29/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 29/100 — VAL   ===\n",
      "[NXT 29/100] loss/tok 2.0904/2.0493 | ppl 8.09/7.76\n",
      "\n",
      "=== BiDir Next-Char Pretrain 30/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 30/100 — VAL   ===\n",
      "[NXT 30/100] loss/tok 2.0992/2.0410 | ppl 8.16/7.70\n",
      "\n",
      "=== BiDir Next-Char Pretrain 31/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 31/100 — VAL   ===\n",
      "[NXT 31/100] loss/tok 2.0780/2.0271 | ppl 7.99/7.59\n",
      "\n",
      "=== BiDir Next-Char Pretrain 32/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 32/100 — VAL   ===\n",
      "[NXT 32/100] loss/tok 2.0750/2.0426 | ppl 7.96/7.71\n",
      "\n",
      "=== BiDir Next-Char Pretrain 33/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 33/100 — VAL   ===\n",
      "[NXT 33/100] loss/tok 2.0810/2.0490 | ppl 8.01/7.76\n",
      "\n",
      "=== BiDir Next-Char Pretrain 34/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 34/100 — VAL   ===\n",
      "[NXT 34/100] loss/tok 2.0820/2.0365 | ppl 8.02/7.66\n",
      "\n",
      "=== BiDir Next-Char Pretrain 35/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 35/100 — VAL   ===\n",
      "[NXT 35/100] loss/tok 2.0678/2.0257 | ppl 7.91/7.58\n",
      "\n",
      "=== BiDir Next-Char Pretrain 36/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 36/100 — VAL   ===\n",
      "[NXT 36/100] loss/tok 2.0604/2.0350 | ppl 7.85/7.65\n",
      "\n",
      "=== BiDir Next-Char Pretrain 37/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 37/100 — VAL   ===\n",
      "[NXT 37/100] loss/tok 2.0780/2.0047 | ppl 7.99/7.42\n",
      "\n",
      "=== BiDir Next-Char Pretrain 38/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 38/100 — VAL   ===\n",
      "[NXT 38/100] loss/tok 2.0673/2.0323 | ppl 7.90/7.63\n",
      "\n",
      "=== BiDir Next-Char Pretrain 39/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 39/100 — VAL   ===\n",
      "[NXT 39/100] loss/tok 2.0685/2.0228 | ppl 7.91/7.56\n",
      "\n",
      "=== BiDir Next-Char Pretrain 40/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 40/100 — VAL   ===\n",
      "[NXT 40/100] loss/tok 2.0550/2.0149 | ppl 7.81/7.50\n",
      "\n",
      "=== BiDir Next-Char Pretrain 41/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 41/100 — VAL   ===\n",
      "[NXT 41/100] loss/tok 2.0532/2.0481 | ppl 7.79/7.75\n",
      "\n",
      "=== BiDir Next-Char Pretrain 42/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 42/100 — VAL   ===\n",
      "[NXT 42/100] loss/tok 2.0501/1.9876 | ppl 7.77/7.30\n",
      "\n",
      "=== BiDir Next-Char Pretrain 43/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 43/100 — VAL   ===\n",
      "[NXT 43/100] loss/tok 2.0569/1.9929 | ppl 7.82/7.34\n",
      "\n",
      "=== BiDir Next-Char Pretrain 44/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 44/100 — VAL   ===\n",
      "[NXT 44/100] loss/tok 2.0475/2.0164 | ppl 7.75/7.51\n",
      "\n",
      "=== BiDir Next-Char Pretrain 45/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 45/100 — VAL   ===\n",
      "[NXT 45/100] loss/tok 2.0567/1.9986 | ppl 7.82/7.38\n",
      "\n",
      "=== BiDir Next-Char Pretrain 46/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 46/100 — VAL   ===\n",
      "[NXT 46/100] loss/tok 2.0457/2.0106 | ppl 7.73/7.47\n",
      "\n",
      "=== BiDir Next-Char Pretrain 47/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 47/100 — VAL   ===\n",
      "[NXT 47/100] loss/tok 2.0455/2.0055 | ppl 7.73/7.43\n",
      "\n",
      "=== BiDir Next-Char Pretrain 48/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 48/100 — VAL   ===\n",
      "[NXT 48/100] loss/tok 2.0531/1.9797 | ppl 7.79/7.24\n",
      "\n",
      "=== BiDir Next-Char Pretrain 49/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 49/100 — VAL   ===\n",
      "[NXT 49/100] loss/tok 2.0332/2.0066 | ppl 7.64/7.44\n",
      "\n",
      "=== BiDir Next-Char Pretrain 50/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 50/100 — VAL   ===\n",
      "[NXT 50/100] loss/tok 2.0439/1.9892 | ppl 7.72/7.31\n",
      "\n",
      "=== BiDir Next-Char Pretrain 51/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 51/100 — VAL   ===\n",
      "[NXT 51/100] loss/tok 2.0262/2.0092 | ppl 7.58/7.46\n",
      "\n",
      "=== BiDir Next-Char Pretrain 52/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 52/100 — VAL   ===\n",
      "[NXT 52/100] loss/tok 2.0373/1.9846 | ppl 7.67/7.28\n",
      "\n",
      "=== BiDir Next-Char Pretrain 53/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 53/100 — VAL   ===\n",
      "[NXT 53/100] loss/tok 2.0332/1.9927 | ppl 7.64/7.34\n",
      "\n",
      "=== BiDir Next-Char Pretrain 54/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 54/100 — VAL   ===\n",
      "[NXT 54/100] loss/tok 2.0323/1.9869 | ppl 7.63/7.29\n",
      "\n",
      "=== BiDir Next-Char Pretrain 55/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 55/100 — VAL   ===\n",
      "[NXT 55/100] loss/tok 2.0260/2.0075 | ppl 7.58/7.44\n",
      "\n",
      "=== BiDir Next-Char Pretrain 56/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 56/100 — VAL   ===\n",
      "[NXT 56/100] loss/tok 2.0315/1.9921 | ppl 7.63/7.33\n",
      "\n",
      "=== BiDir Next-Char Pretrain 57/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 57/100 — VAL   ===\n",
      "[NXT 57/100] loss/tok 2.0174/2.0035 | ppl 7.52/7.42\n",
      "\n",
      "=== BiDir Next-Char Pretrain 58/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 58/100 — VAL   ===\n",
      "[NXT 58/100] loss/tok 2.0244/1.9819 | ppl 7.57/7.26\n",
      "\n",
      "=== BiDir Next-Char Pretrain 59/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 59/100 — VAL   ===\n",
      "[NXT 59/100] loss/tok 2.0229/1.9853 | ppl 7.56/7.28\n",
      "\n",
      "=== BiDir Next-Char Pretrain 60/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 60/100 — VAL   ===\n",
      "[NXT 60/100] loss/tok 2.0175/1.9773 | ppl 7.52/7.22\n",
      "\n",
      "=== BiDir Next-Char Pretrain 61/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 61/100 — VAL   ===\n",
      "[NXT 61/100] loss/tok 2.0322/1.9669 | ppl 7.63/7.15\n",
      "\n",
      "=== BiDir Next-Char Pretrain 62/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 62/100 — VAL   ===\n",
      "[NXT 62/100] loss/tok 2.0209/1.9819 | ppl 7.55/7.26\n",
      "\n",
      "=== BiDir Next-Char Pretrain 63/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 63/100 — VAL   ===\n",
      "[NXT 63/100] loss/tok 2.0192/1.9654 | ppl 7.53/7.14\n",
      "\n",
      "=== BiDir Next-Char Pretrain 64/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 64/100 — VAL   ===\n",
      "[NXT 64/100] loss/tok 2.0127/1.9668 | ppl 7.48/7.15\n",
      "\n",
      "=== BiDir Next-Char Pretrain 65/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 65/100 — VAL   ===\n",
      "[NXT 65/100] loss/tok 2.0258/1.9641 | ppl 7.58/7.13\n",
      "\n",
      "=== BiDir Next-Char Pretrain 66/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 66/100 — VAL   ===\n",
      "[NXT 66/100] loss/tok 2.0126/1.9541 | ppl 7.48/7.06\n",
      "\n",
      "=== BiDir Next-Char Pretrain 67/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 67/100 — VAL   ===\n",
      "[NXT 67/100] loss/tok 2.0126/1.9780 | ppl 7.48/7.23\n",
      "\n",
      "=== BiDir Next-Char Pretrain 68/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 68/100 — VAL   ===\n",
      "[NXT 68/100] loss/tok 2.0074/2.0263 | ppl 7.44/7.59\n",
      "\n",
      "=== BiDir Next-Char Pretrain 69/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 69/100 — VAL   ===\n",
      "[NXT 69/100] loss/tok 2.0176/1.9625 | ppl 7.52/7.12\n",
      "\n",
      "=== BiDir Next-Char Pretrain 70/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 70/100 — VAL   ===\n",
      "[NXT 70/100] loss/tok 2.0153/1.9662 | ppl 7.50/7.14\n",
      "\n",
      "=== BiDir Next-Char Pretrain 71/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 71/100 — VAL   ===\n",
      "[NXT 71/100] loss/tok 2.0017/1.9708 | ppl 7.40/7.18\n",
      "\n",
      "=== BiDir Next-Char Pretrain 72/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 72/100 — VAL   ===\n",
      "[NXT 72/100] loss/tok 2.0080/1.9653 | ppl 7.45/7.14\n",
      "\n",
      "=== BiDir Next-Char Pretrain 73/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 73/100 — VAL   ===\n",
      "[NXT 73/100] loss/tok 2.0148/1.9711 | ppl 7.50/7.18\n",
      "\n",
      "=== BiDir Next-Char Pretrain 74/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 74/100 — VAL   ===\n",
      "[NXT 74/100] loss/tok 2.0117/1.9623 | ppl 7.48/7.12\n",
      "\n",
      "=== BiDir Next-Char Pretrain 75/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 75/100 — VAL   ===\n",
      "[NXT 75/100] loss/tok 2.0020/1.9640 | ppl 7.40/7.13\n",
      "\n",
      "=== BiDir Next-Char Pretrain 76/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 76/100 — VAL   ===\n",
      "[NXT 76/100] loss/tok 2.0135/1.9779 | ppl 7.49/7.23\n",
      "\n",
      "=== BiDir Next-Char Pretrain 77/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 77/100 — VAL   ===\n",
      "[NXT 77/100] loss/tok 2.0026/2.0005 | ppl 7.41/7.39\n",
      "\n",
      "=== BiDir Next-Char Pretrain 78/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 78/100 — VAL   ===\n",
      "[NXT 78/100] loss/tok 2.0086/1.9774 | ppl 7.45/7.22\n",
      "\n",
      "=== BiDir Next-Char Pretrain 79/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 79/100 — VAL   ===\n",
      "[NXT 79/100] loss/tok 1.9844/1.9671 | ppl 7.27/7.15\n",
      "\n",
      "=== BiDir Next-Char Pretrain 80/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 80/100 — VAL   ===\n",
      "[NXT 80/100] loss/tok 2.0077/1.9544 | ppl 7.45/7.06\n",
      "\n",
      "=== BiDir Next-Char Pretrain 81/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 81/100 — VAL   ===\n",
      "[NXT 81/100] loss/tok 1.9962/1.9639 | ppl 7.36/7.13\n",
      "\n",
      "=== BiDir Next-Char Pretrain 82/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 82/100 — VAL   ===\n",
      "[NXT 82/100] loss/tok 2.0101/1.9488 | ppl 7.46/7.02\n",
      "\n",
      "=== BiDir Next-Char Pretrain 83/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 83/100 — VAL   ===\n",
      "[NXT 83/100] loss/tok 1.9934/1.9463 | ppl 7.34/7.00\n",
      "\n",
      "=== BiDir Next-Char Pretrain 84/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 84/100 — VAL   ===\n",
      "[NXT 84/100] loss/tok 1.9835/1.9759 | ppl 7.27/7.21\n",
      "\n",
      "=== BiDir Next-Char Pretrain 85/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 85/100 — VAL   ===\n",
      "[NXT 85/100] loss/tok 1.9967/1.9518 | ppl 7.36/7.04\n",
      "\n",
      "=== BiDir Next-Char Pretrain 86/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 86/100 — VAL   ===\n",
      "[NXT 86/100] loss/tok 1.9939/1.9438 | ppl 7.34/6.99\n",
      "\n",
      "=== BiDir Next-Char Pretrain 87/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 87/100 — VAL   ===\n",
      "[NXT 87/100] loss/tok 1.9893/1.9641 | ppl 7.31/7.13\n",
      "\n",
      "=== BiDir Next-Char Pretrain 88/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 88/100 — VAL   ===\n",
      "[NXT 88/100] loss/tok 1.9975/1.9422 | ppl 7.37/6.97\n",
      "\n",
      "=== BiDir Next-Char Pretrain 89/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 89/100 — VAL   ===\n",
      "[NXT 89/100] loss/tok 1.9790/1.9630 | ppl 7.24/7.12\n",
      "\n",
      "=== BiDir Next-Char Pretrain 90/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 90/100 — VAL   ===\n",
      "[NXT 90/100] loss/tok 1.9934/1.9660 | ppl 7.34/7.14\n",
      "\n",
      "=== BiDir Next-Char Pretrain 91/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 91/100 — VAL   ===\n",
      "[NXT 91/100] loss/tok 2.0013/1.9523 | ppl 7.40/7.04\n",
      "\n",
      "=== BiDir Next-Char Pretrain 92/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 92/100 — VAL   ===\n",
      "[NXT 92/100] loss/tok 1.9887/1.9552 | ppl 7.31/7.07\n",
      "\n",
      "=== BiDir Next-Char Pretrain 93/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 93/100 — VAL   ===\n",
      "[NXT 93/100] loss/tok 1.9885/1.9335 | ppl 7.30/6.91\n",
      "\n",
      "=== BiDir Next-Char Pretrain 94/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 94/100 — VAL   ===\n",
      "[NXT 94/100] loss/tok 1.9673/1.9607 | ppl 7.15/7.10\n",
      "\n",
      "=== BiDir Next-Char Pretrain 95/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 95/100 — VAL   ===\n",
      "[NXT 95/100] loss/tok 1.9881/1.9324 | ppl 7.30/6.91\n",
      "\n",
      "=== BiDir Next-Char Pretrain 96/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 96/100 — VAL   ===\n",
      "[NXT 96/100] loss/tok 1.9657/1.9594 | ppl 7.14/7.09\n",
      "\n",
      "=== BiDir Next-Char Pretrain 97/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 97/100 — VAL   ===\n",
      "[NXT 97/100] loss/tok 1.9806/1.9263 | ppl 7.25/6.86\n",
      "\n",
      "=== BiDir Next-Char Pretrain 98/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 98/100 — VAL   ===\n",
      "[NXT 98/100] loss/tok 1.9869/1.9342 | ppl 7.29/6.92\n",
      "\n",
      "=== BiDir Next-Char Pretrain 99/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 99/100 — VAL   ===\n",
      "[NXT 99/100] loss/tok 1.9832/1.9342 | ppl 7.27/6.92\n",
      "\n",
      "=== BiDir Next-Char Pretrain 100/100 — TRAIN ===\n",
      "=== BiDir Next-Char Pretrain 100/100 — VAL   ===\n",
      "[NXT 100/100] loss/tok 1.9753/1.9453 | ppl 7.21/7.00\n",
      "Saved → m6_bidir_pretrained.pt\n"
     ]
    }
   ],
   "source": [
    "def letters_to_ids(word):\n",
    "    # 'a'..'z' → 3..28\n",
    "    return np.asarray([tok.letter2id[ch] for ch in word], dtype=np.int64)\n",
    "\n",
    "def causal_mask(T, device):\n",
    "    # True=masked (future positions)\n",
    "    return torch.triu(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "def causal_bidir_batches_by_len(words_by_len, batch_size=1024, shuffle=True, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    lengths = list(words_by_len.keys())\n",
    "    if shuffle: rng.shuffle(lengths)\n",
    "    for L in lengths:\n",
    "        idxs = words_by_len[L]\n",
    "        if shuffle: rng.shuffle(idxs)\n",
    "\n",
    "        # Pre-encode forward and reversed sequences (len>=2 only)\n",
    "        forward, reverse = [], []\n",
    "        for i in idxs:\n",
    "            w = all_words[i]\n",
    "            if len(w) < 2: continue\n",
    "            ids = letters_to_ids(w)\n",
    "            rid = ids[::-1]\n",
    "            forward.append(ids)\n",
    "            reverse.append(rid)\n",
    "\n",
    "        # Interleave forward+reverse to improve mixing\n",
    "        pairs = list(zip(forward, reverse))\n",
    "        if shuffle: rng.shuffle(pairs)\n",
    "\n",
    "        # Build batches\n",
    "        for s in range(0, len(pairs), batch_size):\n",
    "            chunk = pairs[s:s+batch_size]\n",
    "            if not chunk: continue\n",
    "            fwd = [p[0] for p in chunk]\n",
    "            rev = [p[1] for p in chunk]\n",
    "\n",
    "            # Inputs/targets for next-char: ([:-1] → [1:])\n",
    "            Xf = np.stack([x[:-1] for x in fwd], axis=0)\n",
    "            Yf = np.stack([x[1:]  for x in fwd], axis=0)\n",
    "            Xr = np.stack([x[:-1] for x in rev], axis=0)\n",
    "            Yr = np.stack([x[1:]  for x in rev], axis=0)\n",
    "\n",
    "            # Concatenate forward + reverse along batch dim\n",
    "            X = np.concatenate([Xf, Xr], axis=0)\n",
    "            Y = np.concatenate([Yf, Yr], axis=0)\n",
    "\n",
    "            yield (\n",
    "                torch.from_numpy(X),         # [2B, T-1] ids in 3..28\n",
    "                torch.from_numpy(Y) - 3,     # [2B, T-1] -> 0..25\n",
    "                L\n",
    "            )\n",
    "\n",
    "def pretrain_next_letter_bidir(model, train_by_len, val_by_len,\n",
    "                               epochs=PRE_EPOCHS, batch_size=PRE_BATCH,\n",
    "                               lr=PRE_LR, log_every=200):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    def run_epoch(by_len, train=True):\n",
    "        model.train(train)\n",
    "        tot_tok, tot_loss, steps = 0, 0.0, 0\n",
    "        gen = causal_bidir_batches_by_len(by_len, batch_size=batch_size,\n",
    "                                          shuffle=True, seed=np.random.randint(10**9))\n",
    "        for X, Y, L in gen:\n",
    "            X, Y = X.to(DEVICE), Y.to(DEVICE)  # [B, T]\n",
    "            B, T = X.shape\n",
    "            attn = causal_mask(T, X.device)\n",
    "            with torch.set_grad_enabled(train):\n",
    "                logits = model.forward_per_pos(X, attn)         # [B,T,26]\n",
    "                loss = F.cross_entropy(logits.reshape(-1,26), Y.reshape(-1))\n",
    "                if train:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    opt.step()\n",
    "            # bookkeeping\n",
    "            ntok = Y.numel()\n",
    "            tot_tok  += ntok\n",
    "            tot_loss += loss.item() * ntok\n",
    "            steps += 1\n",
    "            if log_every and steps % log_every == 0:\n",
    "                ppl = float(np.exp(tot_loss / max(tot_tok,1)))\n",
    "                print(f\"  [{'train' if train else 'val'}] step {steps:>5} | len={L:<2} | loss/tok={tot_loss/tot_tok:.4f} | ppl={ppl:.2f}\")\n",
    "        avg_loss = tot_loss / max(tot_tok,1)\n",
    "        return avg_loss, float(np.exp(avg_loss))\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        print(f\"\\n=== BiDir Next-Char Pretrain {ep}/{epochs} — TRAIN ===\")\n",
    "        tr_loss, tr_ppl = run_epoch(train_by_len, train=True)\n",
    "        print(f\"=== BiDir Next-Char Pretrain {ep}/{epochs} — VAL   ===\")\n",
    "        va_loss, va_ppl = run_epoch(val_by_len,   train=False)\n",
    "        print(f\"[NXT {ep}/{epochs}] loss/tok {tr_loss:.4f}/{va_loss:.4f} | ppl {tr_ppl:.2f}/{va_ppl:.2f}\")\n",
    "\n",
    "    ckpt_pre = {\n",
    "        \"config\": {\"vocab_size\": VOCAB_SIZE, \"d_model\": DMODEL, \"n_heads\": N_HEADS,\n",
    "                   \"n_layers\": N_LAYERS, \"d_ff\": D_FF, \"max_len\": MAX_LEN},\n",
    "        \"state_dict\": model.state_dict(),\n",
    "    }\n",
    "    torch.save(ckpt_pre, \"m6_bidir_pretrained.pt\")\n",
    "    print(\"Saved → m6_bidir_pretrained.pt\")\n",
    "    return model\n",
    "\n",
    "# RUN PRETRAIN (forward + reversed)\n",
    "model = pretrain_next_letter_bidir(model, train_by_len, val_by_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1dcd0-ab1e-4867-9e90-4cc640b87f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16abfa16-578c-4eac-838a-f247e557d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (original): total=221617 | short(1-5)=18583 | sample lens: {1: 17, 2: 259, 3: 2157, 4: 5157, 5: 10993, 6: 19079, 7: 25243, 8: 29686, 9: 30136, 10: 26291}\n",
      "train (short-boosted): total=258783 | short(1-5)=55749 | sample lens: {1: 49, 2: 772, 3: 6431, 4: 15397, 5: 33100, 6: 19079, 7: 25243, 8: 29686, 9: 30136, 10: 26291}\n",
      "Building Hangman states…\n",
      "train states total: 7167888\n",
      "val   states total: 42194\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# After pretraining: oversample ONLY short words (len 1–5)\n",
    "# =========================\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np, random\n",
    "\n",
    "# Keep your existing PERMS_PER_WORD\n",
    "# PERMS_PER_WORD = 3\n",
    "\n",
    "# Define the \"short\" bucket (1..5)\n",
    "SHORT_BUCKET = (1, 5)\n",
    "SHORT_BUCKET_MULT = 3   # <- duplicate 3x total (original + 2x extra). Tweak as you like.\n",
    "\n",
    "def in_short_bucket(L: int) -> bool:\n",
    "    return SHORT_BUCKET[0] <= L <= SHORT_BUCKET[1]\n",
    "\n",
    "def summarize_len_bins(indices, label=\"\"):\n",
    "    cnt = Counter(len(all_words[i]) for i in indices)\n",
    "    small = sum(v for L,v in cnt.items() if in_short_bucket(L))\n",
    "    print(f\"{label}: total={len(indices)} | short(1-5)={small} | sample lens:\",\n",
    "          dict(list(sorted(cnt.items()))[:10]))\n",
    "\n",
    "def states_for_word(word, perms_per_word=1):\n",
    "    L = len(word)\n",
    "    uniq = sorted(set(word))\n",
    "    n = len(uniq)\n",
    "    out = []\n",
    "    for _ in range(perms_per_word):\n",
    "        rnd = uniq[:]\n",
    "        random.shuffle(rnd)\n",
    "        revealed = set()\n",
    "        for k in range(n):\n",
    "            pattern = \"\".join(ch if ch in revealed else \"_\" for ch in word)\n",
    "            remaining = [ch for ch in rnd if ch not in revealed]\n",
    "            if remaining:\n",
    "                mask = np.zeros(26, dtype=np.float32)\n",
    "                for ch in remaining:\n",
    "                    mask[ord(ch)-97] = 1.0\n",
    "                ids = tok.with_cls(tok.encode_board(pattern))\n",
    "                out.append((ids, mask, L))\n",
    "            revealed.add(rnd[k])\n",
    "    return out\n",
    "\n",
    "def build_states_from_indices(indices, perms_per_word):\n",
    "    by_len = defaultdict(list)\n",
    "    for i in indices:\n",
    "        w = all_words[i]\n",
    "        for ids, mask, L in states_for_word(w, perms_per_word=perms_per_word):\n",
    "            by_len[L].append((ids, mask))\n",
    "    return by_len\n",
    "\n",
    "# 1) Start from your original split\n",
    "train_idx_list = list(train_idx)\n",
    "\n",
    "# 2) Split into short vs others\n",
    "short_idxs  = [i for i in train_idx_list if in_short_bucket(len(all_words[i]))]\n",
    "other_idxs  = [i for i in train_idx_list if not in_short_bucket(len(all_words[i]))]\n",
    "\n",
    "summarize_len_bins(train_idx_list, \"train (original)\")\n",
    "\n",
    "# 3) Oversample ONLY the short bucket by a multiplicative factor\n",
    "if short_idxs and SHORT_BUCKET_MULT > 1:\n",
    "    extra = random.choices(short_idxs, k=(SHORT_BUCKET_MULT - 1) * len(short_idxs))\n",
    "    train_idx_aug = other_idxs + short_idxs + extra\n",
    "else:\n",
    "    train_idx_aug = train_idx_list[:]\n",
    "\n",
    "random.shuffle(train_idx_aug)\n",
    "summarize_len_bins(train_idx_aug, \"train (short-boosted)\")\n",
    "\n",
    "# 4) Build states: train from the augmented indices; val stays deterministic\n",
    "print(\"Building Hangman states…\")\n",
    "train_states = build_states_from_indices(train_idx_aug, perms_per_word=PERMS_PER_WORD)\n",
    "val_states   = build_states_from_indices(list(val_idx), perms_per_word=1)\n",
    "\n",
    "print(\"train states total:\", sum(len(v) for v in train_states.values()))\n",
    "print(\"val   states total:\", sum(len(v) for v in val_states.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a50b28-ba01-45d5-9f1a-223db3924c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Finetune (your existing loop; I just ensure lr uses FT_LR)\n",
    "def hm_batches_from_states(by_len, batch_size=FT_BATCH, shuffle=True):\n",
    "    lengths = list(by_len.keys())\n",
    "    rng = random.Random(SEED)\n",
    "    if shuffle:\n",
    "        for L in lengths: rng.shuffle(by_len[L])\n",
    "        rng.shuffle(lengths)\n",
    "    for L in lengths:\n",
    "        bucket = by_len[L]\n",
    "        for s in range(0, len(bucket), batch_size):\n",
    "            chunk = bucket[s:s+batch_size]\n",
    "            xs_np = np.asarray([x for (x,_) in chunk], dtype=np.int64)\n",
    "            ms_np = np.asarray([m for (_,m) in chunk], dtype=np.float32)\n",
    "            yield torch.from_numpy(xs_np), torch.from_numpy(ms_np), L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "064a5260-179a-4902-a69f-85c2452db861",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b531ffc-449a-43e1-901f-850030c2f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights from m10.pt onto cuda.\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_model(path=\"m9.pt\", device=None):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "\n",
    "    # read config if present, otherwise use your defaults\n",
    "    cfg = ckpt.get(\"config\", {}) if isinstance(ckpt, dict) else {}\n",
    "    model = TinyCharTransformer(\n",
    "        vocab_size=cfg.get(\"vocab_size\", 29),\n",
    "        d_model=cfg.get(\"d_model\", 256),\n",
    "        n_heads=cfg.get(\"n_heads\", 8),\n",
    "        n_layers=cfg.get(\"n_layers\", 4),\n",
    "        d_ff=cfg.get(\"d_ff\", 512),\n",
    "        max_len=cfg.get(\"max_len\", 32),\n",
    "    )\n",
    "\n",
    "    # handle both wrapped {\"state_dict\": ...} and raw state_dict checkpoints\n",
    "    state_dict = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[load note] missing keys: {missing}\\n[load note] unexpected keys: {unexpected}\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()  # set to eval; switch to train() before finetune\n",
    "    print(f\"Loaded pretrained weights from {path} onto {device}.\")\n",
    "    return model, device\n",
    "\n",
    "# usage:\n",
    "model, DEVICE = load_pretrained_model(\"m10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76435eb0-a596-46e3-a934-58ddb2f1c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hangman Finetune 1/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2795 | Hit@1=0.869\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2534 | Hit@1=0.881\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2431 | Hit@1=0.885\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2910 | Hit@1=0.865\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3223 | Hit@1=0.852\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4138 | Hit@1=0.818\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4374 | Hit@1=0.807\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.5036 | Hit@1=0.779\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5875 | Hit@1=0.745\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.6082 | Hit@1=0.736\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6453 | Hit@1=0.721\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6792 | Hit@1=0.707\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6842 | Hit@1=0.705\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.7046 | Hit@1=0.696\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6998 | Hit@1=0.698\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6780 | Hit@1=0.706\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6530 | Hit@1=0.717\n",
      "=== Hangman Finetune 1/100 — VAL   ===\n",
      "[HMG 1/100] loss 0.6361/0.5663 | Hit@1 0.724/0.752\n",
      "\n",
      "=== Hangman Finetune 2/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2774 | Hit@1=0.870\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2507 | Hit@1=0.882\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2406 | Hit@1=0.886\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2885 | Hit@1=0.866\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3197 | Hit@1=0.853\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4112 | Hit@1=0.819\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4349 | Hit@1=0.808\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.5007 | Hit@1=0.780\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5845 | Hit@1=0.746\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.6050 | Hit@1=0.737\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6424 | Hit@1=0.722\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6763 | Hit@1=0.708\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6811 | Hit@1=0.706\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.7016 | Hit@1=0.697\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6970 | Hit@1=0.699\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6752 | Hit@1=0.707\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6502 | Hit@1=0.718\n",
      "=== Hangman Finetune 2/100 — VAL   ===\n",
      "[HMG 2/100] loss 0.6335/0.5649 | Hit@1 0.725/0.753\n",
      "\n",
      "=== Hangman Finetune 3/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2764 | Hit@1=0.870\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2492 | Hit@1=0.882\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2393 | Hit@1=0.886\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2868 | Hit@1=0.866\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3182 | Hit@1=0.853\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4095 | Hit@1=0.819\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4328 | Hit@1=0.808\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4987 | Hit@1=0.781\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5824 | Hit@1=0.746\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.6032 | Hit@1=0.737\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6403 | Hit@1=0.723\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6743 | Hit@1=0.709\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6792 | Hit@1=0.706\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6997 | Hit@1=0.698\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6951 | Hit@1=0.699\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6734 | Hit@1=0.708\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6485 | Hit@1=0.719\n",
      "=== Hangman Finetune 3/100 — VAL   ===\n",
      "[HMG 3/100] loss 0.6318/0.5648 | Hit@1 0.726/0.753\n",
      "\n",
      "=== Hangman Finetune 4/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2756 | Hit@1=0.871\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2485 | Hit@1=0.883\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2386 | Hit@1=0.887\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2860 | Hit@1=0.867\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3173 | Hit@1=0.854\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4086 | Hit@1=0.820\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4320 | Hit@1=0.809\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4977 | Hit@1=0.781\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5813 | Hit@1=0.747\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.6020 | Hit@1=0.738\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6392 | Hit@1=0.723\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6730 | Hit@1=0.710\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6782 | Hit@1=0.707\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6986 | Hit@1=0.698\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6940 | Hit@1=0.700\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6723 | Hit@1=0.708\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6473 | Hit@1=0.719\n",
      "=== Hangman Finetune 4/100 — VAL   ===\n",
      "[HMG 4/100] loss 0.6307/0.5641 | Hit@1 0.726/0.753\n",
      "\n",
      "=== Hangman Finetune 5/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2728 | Hit@1=0.871\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2470 | Hit@1=0.883\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2376 | Hit@1=0.887\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2856 | Hit@1=0.867\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3166 | Hit@1=0.854\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4076 | Hit@1=0.820\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4312 | Hit@1=0.809\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4968 | Hit@1=0.782\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5802 | Hit@1=0.747\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.6011 | Hit@1=0.738\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6382 | Hit@1=0.723\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6722 | Hit@1=0.710\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6771 | Hit@1=0.707\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6976 | Hit@1=0.698\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6929 | Hit@1=0.700\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6712 | Hit@1=0.709\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6463 | Hit@1=0.719\n",
      "=== Hangman Finetune 5/100 — VAL   ===\n",
      "[HMG 5/100] loss 0.6296/0.5643 | Hit@1 0.726/0.754\n",
      "\n",
      "=== Hangman Finetune 6/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2728 | Hit@1=0.872\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2465 | Hit@1=0.883\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2365 | Hit@1=0.888\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2841 | Hit@1=0.868\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3155 | Hit@1=0.854\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4070 | Hit@1=0.820\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4302 | Hit@1=0.810\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4958 | Hit@1=0.782\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5792 | Hit@1=0.748\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.6000 | Hit@1=0.739\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6372 | Hit@1=0.724\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6709 | Hit@1=0.710\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6760 | Hit@1=0.707\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6966 | Hit@1=0.699\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6919 | Hit@1=0.700\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6702 | Hit@1=0.709\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6453 | Hit@1=0.720\n",
      "=== Hangman Finetune 6/100 — VAL   ===\n",
      "[HMG 6/100] loss 0.6287/0.5641 | Hit@1 0.727/0.753\n",
      "\n",
      "=== Hangman Finetune 7/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2725 | Hit@1=0.872\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2458 | Hit@1=0.884\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2360 | Hit@1=0.888\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2838 | Hit@1=0.868\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3151 | Hit@1=0.854\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4062 | Hit@1=0.820\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4292 | Hit@1=0.810\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4948 | Hit@1=0.782\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5781 | Hit@1=0.748\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5990 | Hit@1=0.739\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6360 | Hit@1=0.724\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6699 | Hit@1=0.711\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6749 | Hit@1=0.708\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6955 | Hit@1=0.699\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6909 | Hit@1=0.701\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6692 | Hit@1=0.709\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6444 | Hit@1=0.720\n",
      "=== Hangman Finetune 7/100 — VAL   ===\n",
      "[HMG 7/100] loss 0.6277/0.5638 | Hit@1 0.727/0.753\n",
      "\n",
      "=== Hangman Finetune 8/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2722 | Hit@1=0.872\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2455 | Hit@1=0.884\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2358 | Hit@1=0.888\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2829 | Hit@1=0.868\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3143 | Hit@1=0.855\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4053 | Hit@1=0.821\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4287 | Hit@1=0.810\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4942 | Hit@1=0.782\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5775 | Hit@1=0.748\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5983 | Hit@1=0.739\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6354 | Hit@1=0.724\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6695 | Hit@1=0.711\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6744 | Hit@1=0.708\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6949 | Hit@1=0.699\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6904 | Hit@1=0.701\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6687 | Hit@1=0.709\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6439 | Hit@1=0.720\n",
      "=== Hangman Finetune 8/100 — VAL   ===\n",
      "[HMG 8/100] loss 0.6272/0.5639 | Hit@1 0.727/0.754\n",
      "\n",
      "=== Hangman Finetune 9/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2721 | Hit@1=0.871\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2456 | Hit@1=0.884\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2357 | Hit@1=0.888\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2829 | Hit@1=0.868\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3140 | Hit@1=0.855\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4052 | Hit@1=0.821\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4283 | Hit@1=0.810\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4936 | Hit@1=0.783\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5768 | Hit@1=0.748\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5976 | Hit@1=0.739\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6348 | Hit@1=0.725\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6687 | Hit@1=0.711\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6737 | Hit@1=0.708\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6943 | Hit@1=0.699\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6896 | Hit@1=0.701\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6680 | Hit@1=0.710\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6432 | Hit@1=0.720\n",
      "=== Hangman Finetune 9/100 — VAL   ===\n",
      "[HMG 9/100] loss 0.6265/0.5638 | Hit@1 0.727/0.754\n",
      "\n",
      "=== Hangman Finetune 10/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2713 | Hit@1=0.873\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2449 | Hit@1=0.884\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2351 | Hit@1=0.888\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2825 | Hit@1=0.868\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3134 | Hit@1=0.855\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4045 | Hit@1=0.821\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4277 | Hit@1=0.810\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4930 | Hit@1=0.783\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5762 | Hit@1=0.749\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5970 | Hit@1=0.740\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6342 | Hit@1=0.725\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6681 | Hit@1=0.711\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6731 | Hit@1=0.709\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6936 | Hit@1=0.700\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6890 | Hit@1=0.701\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6674 | Hit@1=0.710\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6425 | Hit@1=0.721\n",
      "=== Hangman Finetune 10/100 — VAL   ===\n",
      "[HMG 10/100] loss 0.6259/0.5639 | Hit@1 0.728/0.754\n",
      "\n",
      "=== Hangman Finetune 11/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2709 | Hit@1=0.872\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2447 | Hit@1=0.884\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2348 | Hit@1=0.889\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2824 | Hit@1=0.868\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3131 | Hit@1=0.855\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4038 | Hit@1=0.821\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4270 | Hit@1=0.811\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4922 | Hit@1=0.783\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5753 | Hit@1=0.749\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5961 | Hit@1=0.740\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6332 | Hit@1=0.725\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6671 | Hit@1=0.712\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6722 | Hit@1=0.709\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6928 | Hit@1=0.700\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6883 | Hit@1=0.702\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6666 | Hit@1=0.710\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6418 | Hit@1=0.721\n",
      "=== Hangman Finetune 11/100 — VAL   ===\n",
      "[HMG 11/100] loss 0.6251/0.5637 | Hit@1 0.728/0.754\n",
      "\n",
      "=== Hangman Finetune 12/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2705 | Hit@1=0.872\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2436 | Hit@1=0.885\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2334 | Hit@1=0.889\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2812 | Hit@1=0.869\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3121 | Hit@1=0.856\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4031 | Hit@1=0.822\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4264 | Hit@1=0.811\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4916 | Hit@1=0.783\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5747 | Hit@1=0.749\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5957 | Hit@1=0.740\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6328 | Hit@1=0.725\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6667 | Hit@1=0.712\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6716 | Hit@1=0.709\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6922 | Hit@1=0.700\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6878 | Hit@1=0.702\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6661 | Hit@1=0.710\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6413 | Hit@1=0.721\n",
      "=== Hangman Finetune 12/100 — VAL   ===\n",
      "[HMG 12/100] loss 0.6247/0.5636 | Hit@1 0.728/0.754\n",
      "\n",
      "=== Hangman Finetune 13/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2692 | Hit@1=0.873\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2428 | Hit@1=0.885\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2331 | Hit@1=0.889\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2810 | Hit@1=0.869\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3119 | Hit@1=0.856\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4025 | Hit@1=0.822\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4261 | Hit@1=0.811\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4912 | Hit@1=0.784\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5743 | Hit@1=0.749\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5951 | Hit@1=0.740\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6322 | Hit@1=0.726\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6662 | Hit@1=0.712\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6711 | Hit@1=0.709\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6917 | Hit@1=0.700\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6871 | Hit@1=0.702\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6655 | Hit@1=0.711\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6407 | Hit@1=0.721\n",
      "=== Hangman Finetune 13/100 — VAL   ===\n",
      "[HMG 13/100] loss 0.6241/0.5639 | Hit@1 0.728/0.755\n",
      "\n",
      "=== Hangman Finetune 14/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2697 | Hit@1=0.873\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2429 | Hit@1=0.885\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2329 | Hit@1=0.889\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2799 | Hit@1=0.869\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3112 | Hit@1=0.856\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4020 | Hit@1=0.822\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4253 | Hit@1=0.811\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4906 | Hit@1=0.784\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5737 | Hit@1=0.750\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5945 | Hit@1=0.741\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6316 | Hit@1=0.726\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6653 | Hit@1=0.712\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6705 | Hit@1=0.709\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6911 | Hit@1=0.701\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6865 | Hit@1=0.702\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6649 | Hit@1=0.711\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6401 | Hit@1=0.722\n",
      "=== Hangman Finetune 14/100 — VAL   ===\n",
      "[HMG 14/100] loss 0.6235/0.5636 | Hit@1 0.729/0.754\n",
      "\n",
      "=== Hangman Finetune 15/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2693 | Hit@1=0.873\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2429 | Hit@1=0.885\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2329 | Hit@1=0.889\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2802 | Hit@1=0.869\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3113 | Hit@1=0.856\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4019 | Hit@1=0.822\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4252 | Hit@1=0.811\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4905 | Hit@1=0.784\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5732 | Hit@1=0.750\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5942 | Hit@1=0.741\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6313 | Hit@1=0.726\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6651 | Hit@1=0.713\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6701 | Hit@1=0.710\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6908 | Hit@1=0.701\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6862 | Hit@1=0.702\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6645 | Hit@1=0.711\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6398 | Hit@1=0.722\n",
      "=== Hangman Finetune 15/100 — VAL   ===\n",
      "[HMG 15/100] loss 0.6232/0.5636 | Hit@1 0.729/0.755\n",
      "\n",
      "=== Hangman Finetune 16/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2688 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2420 | Hit@1=0.885\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2325 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2795 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3108 | Hit@1=0.856\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4015 | Hit@1=0.822\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4249 | Hit@1=0.811\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4901 | Hit@1=0.784\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5729 | Hit@1=0.750\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5939 | Hit@1=0.741\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6309 | Hit@1=0.726\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6647 | Hit@1=0.713\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6698 | Hit@1=0.710\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6903 | Hit@1=0.701\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6858 | Hit@1=0.703\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6642 | Hit@1=0.711\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6394 | Hit@1=0.722\n",
      "=== Hangman Finetune 16/100 — VAL   ===\n",
      "[HMG 16/100] loss 0.6228/0.5643 | Hit@1 0.729/0.755\n",
      "\n",
      "=== Hangman Finetune 17/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2685 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2421 | Hit@1=0.885\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2324 | Hit@1=0.889\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2794 | Hit@1=0.869\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3106 | Hit@1=0.856\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4014 | Hit@1=0.822\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4246 | Hit@1=0.811\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4898 | Hit@1=0.784\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5725 | Hit@1=0.750\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5933 | Hit@1=0.741\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6305 | Hit@1=0.726\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6643 | Hit@1=0.713\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6693 | Hit@1=0.710\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6900 | Hit@1=0.701\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6854 | Hit@1=0.703\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6638 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6390 | Hit@1=0.722\n",
      "=== Hangman Finetune 17/100 — VAL   ===\n",
      "[HMG 17/100] loss 0.6224/0.5637 | Hit@1 0.729/0.755\n",
      "\n",
      "=== Hangman Finetune 18/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2672 | Hit@1=0.873\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2407 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2309 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2785 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3096 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4007 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4239 | Hit@1=0.812\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4890 | Hit@1=0.785\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5717 | Hit@1=0.750\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5925 | Hit@1=0.741\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6298 | Hit@1=0.727\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6636 | Hit@1=0.713\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6687 | Hit@1=0.710\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6893 | Hit@1=0.701\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6848 | Hit@1=0.703\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6632 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6385 | Hit@1=0.722\n",
      "=== Hangman Finetune 18/100 — VAL   ===\n",
      "[HMG 18/100] loss 0.6219/0.5636 | Hit@1 0.729/0.755\n",
      "\n",
      "=== Hangman Finetune 19/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2678 | Hit@1=0.873\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2407 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2312 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2786 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3094 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4004 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4235 | Hit@1=0.812\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4883 | Hit@1=0.785\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5711 | Hit@1=0.750\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5922 | Hit@1=0.741\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6294 | Hit@1=0.726\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6631 | Hit@1=0.713\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6683 | Hit@1=0.710\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6889 | Hit@1=0.701\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6843 | Hit@1=0.703\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6627 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6379 | Hit@1=0.722\n",
      "=== Hangman Finetune 19/100 — VAL   ===\n",
      "[HMG 19/100] loss 0.6213/0.5641 | Hit@1 0.729/0.755\n",
      "\n",
      "=== Hangman Finetune 20/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2674 | Hit@1=0.873\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2413 | Hit@1=0.885\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2316 | Hit@1=0.889\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2789 | Hit@1=0.869\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3096 | Hit@1=0.856\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.4003 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4234 | Hit@1=0.812\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4884 | Hit@1=0.784\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5712 | Hit@1=0.750\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5920 | Hit@1=0.741\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6292 | Hit@1=0.727\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6630 | Hit@1=0.713\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6681 | Hit@1=0.710\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6887 | Hit@1=0.701\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6842 | Hit@1=0.703\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6626 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6378 | Hit@1=0.722\n",
      "=== Hangman Finetune 20/100 — VAL   ===\n",
      "[HMG 20/100] loss 0.6212/0.5642 | Hit@1 0.729/0.754\n",
      "\n",
      "=== Hangman Finetune 21/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2662 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2402 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2307 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2780 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3091 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3998 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4232 | Hit@1=0.812\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4882 | Hit@1=0.785\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5709 | Hit@1=0.751\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5918 | Hit@1=0.742\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6290 | Hit@1=0.727\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6627 | Hit@1=0.713\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6679 | Hit@1=0.710\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6886 | Hit@1=0.702\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6841 | Hit@1=0.703\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6624 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6377 | Hit@1=0.722\n",
      "=== Hangman Finetune 21/100 — VAL   ===\n",
      "[HMG 21/100] loss 0.6211/0.5640 | Hit@1 0.729/0.756\n",
      "\n",
      "=== Hangman Finetune 22/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2668 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2403 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2305 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2777 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3086 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3993 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4225 | Hit@1=0.812\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4875 | Hit@1=0.785\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5701 | Hit@1=0.751\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5911 | Hit@1=0.742\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6282 | Hit@1=0.727\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6620 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6671 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6878 | Hit@1=0.702\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6832 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6616 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6369 | Hit@1=0.723\n",
      "=== Hangman Finetune 22/100 — VAL   ===\n",
      "[HMG 22/100] loss 0.6203/0.5638 | Hit@1 0.730/0.755\n",
      "\n",
      "=== Hangman Finetune 23/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2667 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2400 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2303 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2777 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3082 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3990 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4223 | Hit@1=0.812\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4873 | Hit@1=0.785\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5697 | Hit@1=0.751\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5906 | Hit@1=0.742\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6278 | Hit@1=0.727\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6616 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6667 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6874 | Hit@1=0.702\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6829 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6612 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6366 | Hit@1=0.723\n",
      "=== Hangman Finetune 23/100 — VAL   ===\n",
      "[HMG 23/100] loss 0.6200/0.5633 | Hit@1 0.730/0.756\n",
      "\n",
      "=== Hangman Finetune 24/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2657 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2395 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2297 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2767 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3080 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3986 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4219 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4868 | Hit@1=0.785\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5694 | Hit@1=0.751\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5902 | Hit@1=0.742\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6275 | Hit@1=0.727\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6613 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6664 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6872 | Hit@1=0.702\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6826 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6610 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6363 | Hit@1=0.723\n",
      "=== Hangman Finetune 24/100 — VAL   ===\n",
      "[HMG 24/100] loss 0.6198/0.5633 | Hit@1 0.730/0.755\n",
      "\n",
      "=== Hangman Finetune 25/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2667 | Hit@1=0.873\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2394 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2297 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2767 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3078 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3985 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4216 | Hit@1=0.812\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4862 | Hit@1=0.785\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5689 | Hit@1=0.751\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5899 | Hit@1=0.742\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6271 | Hit@1=0.727\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6608 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6661 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6867 | Hit@1=0.702\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6822 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6606 | Hit@1=0.712\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6359 | Hit@1=0.723\n",
      "=== Hangman Finetune 25/100 — VAL   ===\n",
      "[HMG 25/100] loss 0.6193/0.5636 | Hit@1 0.730/0.755\n",
      "\n",
      "=== Hangman Finetune 26/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2656 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2388 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2294 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2766 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3076 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3985 | Hit@1=0.823\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4215 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4863 | Hit@1=0.785\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5689 | Hit@1=0.751\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5899 | Hit@1=0.742\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6269 | Hit@1=0.727\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6607 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6659 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6866 | Hit@1=0.702\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6820 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6604 | Hit@1=0.713\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6357 | Hit@1=0.723\n",
      "=== Hangman Finetune 26/100 — VAL   ===\n",
      "[HMG 26/100] loss 0.6191/0.5639 | Hit@1 0.730/0.755\n",
      "\n",
      "=== Hangman Finetune 27/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2656 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2390 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2293 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2764 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3072 | Hit@1=0.857\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3979 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4211 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4859 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5685 | Hit@1=0.751\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5895 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6265 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6602 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6654 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6861 | Hit@1=0.702\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6816 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6600 | Hit@1=0.713\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6353 | Hit@1=0.723\n",
      "=== Hangman Finetune 27/100 — VAL   ===\n",
      "[HMG 27/100] loss 0.6188/0.5634 | Hit@1 0.730/0.755\n",
      "\n",
      "=== Hangman Finetune 28/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2653 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2391 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2292 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2762 | Hit@1=0.870\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3070 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3975 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4206 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4856 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5681 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5889 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6263 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6600 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6651 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6858 | Hit@1=0.702\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6814 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6598 | Hit@1=0.713\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6351 | Hit@1=0.723\n",
      "=== Hangman Finetune 28/100 — VAL   ===\n",
      "[HMG 28/100] loss 0.6186/0.5632 | Hit@1 0.730/0.755\n",
      "\n",
      "=== Hangman Finetune 29/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2657 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2391 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2291 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2765 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3069 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3973 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4207 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4851 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5679 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5888 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6259 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6596 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6647 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6854 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6810 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6594 | Hit@1=0.713\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6347 | Hit@1=0.723\n",
      "=== Hangman Finetune 29/100 — VAL   ===\n",
      "[HMG 29/100] loss 0.6182/0.5635 | Hit@1 0.730/0.755\n",
      "\n",
      "=== Hangman Finetune 30/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2647 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2385 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2290 | Hit@1=0.890\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2760 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3064 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3970 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4199 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4849 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5673 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5884 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6256 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6593 | Hit@1=0.714\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6644 | Hit@1=0.711\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6852 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6807 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6591 | Hit@1=0.713\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6344 | Hit@1=0.723\n",
      "=== Hangman Finetune 30/100 — VAL   ===\n",
      "[HMG 30/100] loss 0.6179/0.5633 | Hit@1 0.730/0.755\n",
      "\n",
      "=== Hangman Finetune 31/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2650 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2386 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2289 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2759 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3067 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3971 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4201 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4850 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5672 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5881 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6252 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6589 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6641 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6850 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6804 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6589 | Hit@1=0.713\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6341 | Hit@1=0.724\n",
      "=== Hangman Finetune 31/100 — VAL   ===\n",
      "[HMG 31/100] loss 0.6176/0.5636 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 32/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2652 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2387 | Hit@1=0.886\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2286 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2755 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3061 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3967 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4198 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4842 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5667 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5877 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6249 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6586 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6637 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6845 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6801 | Hit@1=0.704\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6584 | Hit@1=0.713\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6338 | Hit@1=0.724\n",
      "=== Hangman Finetune 32/100 — VAL   ===\n",
      "[HMG 32/100] loss 0.6173/0.5633 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 33/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2639 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2377 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2282 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2751 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3060 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3966 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4196 | Hit@1=0.813\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4842 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5665 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5874 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6247 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6584 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6635 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6842 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6798 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6583 | Hit@1=0.713\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6335 | Hit@1=0.724\n",
      "=== Hangman Finetune 33/100 — VAL   ===\n",
      "[HMG 33/100] loss 0.6171/0.5636 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 34/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2630 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2374 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2276 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2746 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3055 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3958 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4190 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4839 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5660 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5869 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6241 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6580 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6632 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6840 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6795 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6579 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6332 | Hit@1=0.724\n",
      "=== Hangman Finetune 34/100 — VAL   ===\n",
      "[HMG 34/100] loss 0.6167/0.5635 | Hit@1 0.731/0.755\n",
      "\n",
      "=== Hangman Finetune 35/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2635 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2374 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2278 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2746 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3055 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3960 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4192 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4839 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5662 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5874 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6243 | Hit@1=0.728\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6581 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6633 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6840 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6794 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6579 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6333 | Hit@1=0.724\n",
      "=== Hangman Finetune 35/100 — VAL   ===\n",
      "[HMG 35/100] loss 0.6167/0.5634 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 36/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2644 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2370 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2274 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2742 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3052 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3956 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4187 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4832 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5655 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5866 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6238 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6575 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6627 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6835 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6789 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6574 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6328 | Hit@1=0.724\n",
      "=== Hangman Finetune 36/100 — VAL   ===\n",
      "[HMG 36/100] loss 0.6163/0.5636 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 37/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2646 | Hit@1=0.874\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2373 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2276 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2747 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3051 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3955 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4188 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4835 | Hit@1=0.786\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5655 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5865 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6237 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6573 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6625 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6833 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6789 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6574 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6327 | Hit@1=0.724\n",
      "=== Hangman Finetune 37/100 — VAL   ===\n",
      "[HMG 37/100] loss 0.6162/0.5630 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 38/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2638 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2371 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2273 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2742 | Hit@1=0.871\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3050 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3956 | Hit@1=0.824\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4185 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4830 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5655 | Hit@1=0.752\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5864 | Hit@1=0.743\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6236 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6574 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6624 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6832 | Hit@1=0.703\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6788 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6572 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6326 | Hit@1=0.724\n",
      "=== Hangman Finetune 38/100 — VAL   ===\n",
      "[HMG 38/100] loss 0.6161/0.5634 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 39/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2625 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2368 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2270 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2737 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3047 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3949 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4182 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4827 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5648 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5857 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6230 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6568 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6619 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6826 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6783 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6567 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6321 | Hit@1=0.724\n",
      "=== Hangman Finetune 39/100 — VAL   ===\n",
      "[HMG 39/100] loss 0.6156/0.5634 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 40/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2632 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2367 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2271 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2740 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3049 | Hit@1=0.858\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3951 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4182 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4828 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5650 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5860 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6232 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6570 | Hit@1=0.715\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6621 | Hit@1=0.712\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6829 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6784 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6569 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6322 | Hit@1=0.724\n",
      "=== Hangman Finetune 40/100 — VAL   ===\n",
      "[HMG 40/100] loss 0.6157/0.5638 | Hit@1 0.731/0.756\n",
      "\n",
      "=== Hangman Finetune 41/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2627 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2365 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2264 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2734 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3044 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3947 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4178 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4823 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5643 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5855 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6226 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6562 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6615 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6823 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6778 | Hit@1=0.705\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6563 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6316 | Hit@1=0.725\n",
      "=== Hangman Finetune 41/100 — VAL   ===\n",
      "[HMG 41/100] loss 0.6151/0.5641 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 42/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2629 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2362 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2264 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2731 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3039 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3940 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4175 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4819 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5640 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5852 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6223 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6559 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6611 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6819 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6775 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6559 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6313 | Hit@1=0.725\n",
      "=== Hangman Finetune 42/100 — VAL   ===\n",
      "[HMG 42/100] loss 0.6148/0.5639 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 43/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2631 | Hit@1=0.875\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2362 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2266 | Hit@1=0.891\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2731 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3039 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3944 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4173 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4819 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5639 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5850 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6221 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6558 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6611 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6818 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6774 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6558 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6312 | Hit@1=0.725\n",
      "=== Hangman Finetune 43/100 — VAL   ===\n",
      "[HMG 43/100] loss 0.6147/0.5638 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 44/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2616 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2358 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2260 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2730 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3037 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3939 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4170 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4816 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5637 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5847 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6220 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6557 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6610 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6816 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6773 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6558 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6312 | Hit@1=0.725\n",
      "=== Hangman Finetune 44/100 — VAL   ===\n",
      "[HMG 44/100] loss 0.6147/0.5636 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 45/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2616 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2360 | Hit@1=0.887\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2261 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2726 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3033 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3938 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4169 | Hit@1=0.814\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4813 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5633 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5844 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6215 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6554 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6606 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6814 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6770 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6554 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6308 | Hit@1=0.725\n",
      "=== Hangman Finetune 45/100 — VAL   ===\n",
      "[HMG 45/100] loss 0.6143/0.5638 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 46/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2610 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2353 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2257 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2724 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3033 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3937 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4166 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4811 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5632 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5843 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6215 | Hit@1=0.729\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6552 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6603 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6811 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6767 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6552 | Hit@1=0.714\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6306 | Hit@1=0.725\n",
      "=== Hangman Finetune 46/100 — VAL   ===\n",
      "[HMG 46/100] loss 0.6141/0.5631 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 47/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2605 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2349 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2255 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2723 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3031 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3933 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4167 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4812 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5631 | Hit@1=0.753\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5841 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6213 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6550 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6601 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6809 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6766 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6549 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6304 | Hit@1=0.725\n",
      "=== Hangman Finetune 47/100 — VAL   ===\n",
      "[HMG 47/100] loss 0.6139/0.5638 | Hit@1 0.732/0.755\n",
      "\n",
      "=== Hangman Finetune 48/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2618 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2353 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2254 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2724 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3030 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3932 | Hit@1=0.825\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4164 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4809 | Hit@1=0.787\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5628 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5838 | Hit@1=0.744\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6210 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6546 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6598 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6807 | Hit@1=0.704\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6764 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6549 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6302 | Hit@1=0.725\n",
      "=== Hangman Finetune 48/100 — VAL   ===\n",
      "[HMG 48/100] loss 0.6138/0.5636 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 49/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2607 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2349 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2252 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2720 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3026 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3928 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4159 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4805 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5624 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5836 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6207 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6543 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6596 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6803 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6759 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6544 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6298 | Hit@1=0.725\n",
      "=== Hangman Finetune 49/100 — VAL   ===\n",
      "[HMG 49/100] loss 0.6134/0.5635 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 50/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2608 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2349 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2252 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2722 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3028 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3927 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4161 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4804 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5624 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5835 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6206 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6543 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6595 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6802 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6759 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6543 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6297 | Hit@1=0.725\n",
      "=== Hangman Finetune 50/100 — VAL   ===\n",
      "[HMG 50/100] loss 0.6133/0.5634 | Hit@1 0.732/0.757\n",
      "\n",
      "=== Hangman Finetune 51/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2605 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2347 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2250 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2718 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3026 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3928 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4159 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4803 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5621 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5832 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6203 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6542 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6595 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6802 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6758 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6543 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6297 | Hit@1=0.725\n",
      "=== Hangman Finetune 51/100 — VAL   ===\n",
      "[HMG 51/100] loss 0.6132/0.5633 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 52/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2618 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2347 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2252 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2720 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3025 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3927 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4157 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4801 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5620 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5832 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6204 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6541 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6593 | Hit@1=0.713\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6802 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6757 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6542 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6296 | Hit@1=0.725\n",
      "=== Hangman Finetune 52/100 — VAL   ===\n",
      "[HMG 52/100] loss 0.6132/0.5638 | Hit@1 0.732/0.757\n",
      "\n",
      "=== Hangman Finetune 53/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2592 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2342 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2245 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2710 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3021 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3921 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4153 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4797 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5616 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5828 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6199 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6537 | Hit@1=0.716\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6589 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6797 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6753 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6538 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6292 | Hit@1=0.725\n",
      "=== Hangman Finetune 53/100 — VAL   ===\n",
      "[HMG 53/100] loss 0.6128/0.5633 | Hit@1 0.732/0.756\n",
      "\n",
      "=== Hangman Finetune 54/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2607 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2342 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2243 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2711 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3019 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3920 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4152 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4795 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5615 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5825 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6197 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6535 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6588 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6796 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6752 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6537 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6290 | Hit@1=0.726\n",
      "=== Hangman Finetune 54/100 — VAL   ===\n",
      "[HMG 54/100] loss 0.6126/0.5637 | Hit@1 0.733/0.757\n",
      "\n",
      "=== Hangman Finetune 55/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2601 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2336 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2242 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2709 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3014 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3915 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4146 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4790 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5608 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5819 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6193 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6530 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6582 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6790 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6747 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6532 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6286 | Hit@1=0.726\n",
      "=== Hangman Finetune 55/100 — VAL   ===\n",
      "[HMG 55/100] loss 0.6122/0.5636 | Hit@1 0.733/0.757\n",
      "\n",
      "=== Hangman Finetune 56/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2601 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2341 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2244 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2711 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3017 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3918 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4148 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4796 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5611 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5823 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6195 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6532 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6584 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6793 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6748 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6533 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6287 | Hit@1=0.726\n",
      "=== Hangman Finetune 56/100 — VAL   ===\n",
      "[HMG 56/100] loss 0.6123/0.5638 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 57/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2599 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2338 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2242 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2710 | Hit@1=0.872\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3017 | Hit@1=0.859\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3918 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4148 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4791 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5610 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5821 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6193 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6529 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6582 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6790 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6747 | Hit@1=0.706\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6532 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6286 | Hit@1=0.726\n",
      "=== Hangman Finetune 57/100 — VAL   ===\n",
      "[HMG 57/100] loss 0.6122/0.5636 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 58/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2596 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2337 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2243 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2709 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3016 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3915 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4144 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4790 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5607 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5819 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6190 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6527 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6579 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6788 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6743 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6528 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6282 | Hit@1=0.726\n",
      "=== Hangman Finetune 58/100 — VAL   ===\n",
      "[HMG 58/100] loss 0.6118/0.5635 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 59/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2590 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2334 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2242 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2710 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3015 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3913 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4144 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4789 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5605 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5818 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6187 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6524 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6576 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6785 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6741 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6527 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6281 | Hit@1=0.726\n",
      "=== Hangman Finetune 59/100 — VAL   ===\n",
      "[HMG 59/100] loss 0.6116/0.5634 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 60/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2591 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2335 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2243 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2708 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3013 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3913 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4143 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4785 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5604 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5816 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6187 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6524 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6576 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6785 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6741 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6526 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6280 | Hit@1=0.726\n",
      "=== Hangman Finetune 60/100 — VAL   ===\n",
      "[HMG 60/100] loss 0.6116/0.5638 | Hit@1 0.733/0.757\n",
      "\n",
      "=== Hangman Finetune 61/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2601 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2338 | Hit@1=0.888\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2242 | Hit@1=0.892\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2706 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3012 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3912 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4145 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4784 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5602 | Hit@1=0.754\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5814 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6185 | Hit@1=0.730\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6522 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6573 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6782 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6739 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6524 | Hit@1=0.715\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6278 | Hit@1=0.726\n",
      "=== Hangman Finetune 61/100 — VAL   ===\n",
      "[HMG 61/100] loss 0.6114/0.5638 | Hit@1 0.733/0.757\n",
      "\n",
      "=== Hangman Finetune 62/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2584 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2330 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2239 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2705 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3009 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3911 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4142 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4783 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5600 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5813 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6183 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6519 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6572 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6780 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6737 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6522 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6276 | Hit@1=0.726\n",
      "=== Hangman Finetune 62/100 — VAL   ===\n",
      "[HMG 62/100] loss 0.6112/0.5637 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 63/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2593 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2335 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2237 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2702 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3006 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3905 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4137 | Hit@1=0.815\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4779 | Hit@1=0.788\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5596 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5810 | Hit@1=0.745\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6179 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6515 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6568 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6777 | Hit@1=0.705\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6734 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6520 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6274 | Hit@1=0.726\n",
      "=== Hangman Finetune 63/100 — VAL   ===\n",
      "[HMG 63/100] loss 0.6110/0.5641 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 64/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2594 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2331 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2236 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2699 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3006 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3907 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4134 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4777 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5593 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5804 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6177 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6514 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6567 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6776 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6732 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6517 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6272 | Hit@1=0.726\n",
      "=== Hangman Finetune 64/100 — VAL   ===\n",
      "[HMG 64/100] loss 0.6108/0.5639 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 65/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2598 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2330 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2235 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2703 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3006 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3903 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4135 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4777 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5595 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5806 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6177 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6514 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6567 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6776 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6732 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6516 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6271 | Hit@1=0.726\n",
      "=== Hangman Finetune 65/100 — VAL   ===\n",
      "[HMG 65/100] loss 0.6107/0.5640 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 66/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2586 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2327 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2232 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2702 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3005 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3904 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4135 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4776 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5593 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5807 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6177 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6513 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6565 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6773 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6729 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6514 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6269 | Hit@1=0.726\n",
      "=== Hangman Finetune 66/100 — VAL   ===\n",
      "[HMG 66/100] loss 0.6105/0.5643 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 67/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2603 | Hit@1=0.876\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2332 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2238 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2700 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3006 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3905 | Hit@1=0.826\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4135 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4777 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5593 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5805 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6176 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6513 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6565 | Hit@1=0.714\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6773 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6730 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6515 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6270 | Hit@1=0.726\n",
      "=== Hangman Finetune 67/100 — VAL   ===\n",
      "[HMG 67/100] loss 0.6106/0.5640 | Hit@1 0.733/0.757\n",
      "\n",
      "=== Hangman Finetune 68/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2590 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2323 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2229 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2696 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3001 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3899 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4131 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4774 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5590 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5802 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6173 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6509 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6562 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6771 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6727 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6512 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6267 | Hit@1=0.727\n",
      "=== Hangman Finetune 68/100 — VAL   ===\n",
      "[HMG 68/100] loss 0.6103/0.5642 | Hit@1 0.733/0.757\n",
      "\n",
      "=== Hangman Finetune 69/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2590 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2324 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2230 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2692 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2998 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3899 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4132 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4771 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5588 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5798 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6171 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6508 | Hit@1=0.717\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6560 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6769 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6725 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6511 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6265 | Hit@1=0.726\n",
      "=== Hangman Finetune 69/100 — VAL   ===\n",
      "[HMG 69/100] loss 0.6101/0.5638 | Hit@1 0.733/0.757\n",
      "\n",
      "=== Hangman Finetune 70/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2581 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2318 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2224 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2691 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2996 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3894 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4126 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4768 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5586 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5796 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6170 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6506 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6559 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6767 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6723 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6509 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6264 | Hit@1=0.726\n",
      "=== Hangman Finetune 70/100 — VAL   ===\n",
      "[HMG 70/100] loss 0.6100/0.5640 | Hit@1 0.733/0.756\n",
      "\n",
      "=== Hangman Finetune 71/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2583 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2328 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2231 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2695 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.3000 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3900 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4129 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4770 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5586 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5797 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6168 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6504 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6557 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6766 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6723 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6509 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6263 | Hit@1=0.727\n",
      "=== Hangman Finetune 71/100 — VAL   ===\n",
      "[HMG 71/100] loss 0.6099/0.5640 | Hit@1 0.733/0.757\n",
      "\n",
      "=== Hangman Finetune 72/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2575 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2321 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2224 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2693 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2996 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3895 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4125 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4767 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5583 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5794 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6165 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6500 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6555 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6764 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6720 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6505 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6260 | Hit@1=0.727\n",
      "=== Hangman Finetune 72/100 — VAL   ===\n",
      "[HMG 72/100] loss 0.6096/0.5644 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 73/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2578 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2320 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2224 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2691 | Hit@1=0.873\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2994 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3894 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4124 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4765 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5579 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5791 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6163 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6499 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6552 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6761 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6719 | Hit@1=0.707\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6503 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6258 | Hit@1=0.727\n",
      "=== Hangman Finetune 73/100 — VAL   ===\n",
      "[HMG 73/100] loss 0.6094/0.5642 | Hit@1 0.734/0.756\n",
      "\n",
      "=== Hangman Finetune 74/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2580 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2318 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2221 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2686 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2994 | Hit@1=0.860\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3892 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4123 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4765 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5581 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5791 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6164 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6502 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6554 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6764 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6718 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6504 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6259 | Hit@1=0.727\n",
      "=== Hangman Finetune 74/100 — VAL   ===\n",
      "[HMG 74/100] loss 0.6095/0.5638 | Hit@1 0.734/0.756\n",
      "\n",
      "=== Hangman Finetune 75/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2586 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2318 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2224 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2689 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2995 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3892 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4122 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4764 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5579 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5791 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6162 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6500 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6551 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6760 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6717 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6503 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6257 | Hit@1=0.727\n",
      "=== Hangman Finetune 75/100 — VAL   ===\n",
      "[HMG 75/100] loss 0.6093/0.5643 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 76/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2588 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2318 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2225 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2689 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2992 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3892 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4122 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4761 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5577 | Hit@1=0.755\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5789 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6160 | Hit@1=0.731\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6496 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6550 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6759 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6716 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6501 | Hit@1=0.716\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6256 | Hit@1=0.727\n",
      "=== Hangman Finetune 76/100 — VAL   ===\n",
      "[HMG 76/100] loss 0.6092/0.5640 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 77/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2581 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2314 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2220 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2685 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2991 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3889 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4120 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4760 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5575 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5788 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6160 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6495 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6547 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6755 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6713 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6499 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6254 | Hit@1=0.727\n",
      "=== Hangman Finetune 77/100 — VAL   ===\n",
      "[HMG 77/100] loss 0.6090/0.5644 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 78/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2570 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2316 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2220 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2685 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2990 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3889 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4117 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4758 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5573 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5785 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6157 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6494 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6546 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6755 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6711 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6497 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6252 | Hit@1=0.727\n",
      "=== Hangman Finetune 78/100 — VAL   ===\n",
      "[HMG 78/100] loss 0.6089/0.5640 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 79/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2585 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2317 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2221 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2682 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2988 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3886 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4115 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4758 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5571 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5782 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6154 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6491 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6544 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6752 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6710 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6494 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6249 | Hit@1=0.727\n",
      "=== Hangman Finetune 79/100 — VAL   ===\n",
      "[HMG 79/100] loss 0.6085/0.5644 | Hit@1 0.734/0.756\n",
      "\n",
      "=== Hangman Finetune 80/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2579 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2314 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2221 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2682 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2987 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3884 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4115 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4757 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5570 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5782 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6153 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6489 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6542 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6753 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6708 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6494 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6249 | Hit@1=0.727\n",
      "=== Hangman Finetune 80/100 — VAL   ===\n",
      "[HMG 80/100] loss 0.6085/0.5646 | Hit@1 0.734/0.756\n",
      "\n",
      "=== Hangman Finetune 81/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2565 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2310 | Hit@1=0.889\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2215 | Hit@1=0.893\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2682 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2985 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3881 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4114 | Hit@1=0.816\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4753 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5568 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5779 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6152 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6489 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6542 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6750 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6708 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6493 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6248 | Hit@1=0.727\n",
      "=== Hangman Finetune 81/100 — VAL   ===\n",
      "[HMG 81/100] loss 0.6084/0.5640 | Hit@1 0.734/0.756\n",
      "\n",
      "=== Hangman Finetune 82/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2573 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2309 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2216 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2682 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2986 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3882 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4114 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4754 | Hit@1=0.789\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5568 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5780 | Hit@1=0.746\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6153 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6489 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6542 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6751 | Hit@1=0.706\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6707 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6493 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6248 | Hit@1=0.727\n",
      "=== Hangman Finetune 82/100 — VAL   ===\n",
      "[HMG 82/100] loss 0.6084/0.5641 | Hit@1 0.734/0.758\n",
      "\n",
      "=== Hangman Finetune 83/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2578 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2313 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2213 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2679 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2981 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3879 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4109 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4752 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5566 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5776 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6149 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6485 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6539 | Hit@1=0.715\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6748 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6705 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6490 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6246 | Hit@1=0.727\n",
      "=== Hangman Finetune 83/100 — VAL   ===\n",
      "[HMG 83/100] loss 0.6082/0.5637 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 84/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2557 | Hit@1=0.879\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2305 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2213 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2677 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2982 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3880 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4109 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4752 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5566 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5779 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6148 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6485 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6538 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6748 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6704 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6490 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6245 | Hit@1=0.727\n",
      "=== Hangman Finetune 84/100 — VAL   ===\n",
      "[HMG 84/100] loss 0.6081/0.5643 | Hit@1 0.734/0.756\n",
      "\n",
      "=== Hangman Finetune 85/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2569 | Hit@1=0.879\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2310 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2214 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2676 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2980 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3878 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4109 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4750 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5563 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5775 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6146 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6482 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6536 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6745 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6703 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6488 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6243 | Hit@1=0.727\n",
      "=== Hangman Finetune 85/100 — VAL   ===\n",
      "[HMG 85/100] loss 0.6080/0.5636 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 86/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2561 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2308 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2210 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2674 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2978 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3876 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4103 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4748 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5560 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5774 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6144 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6481 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6534 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6743 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6701 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6486 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6241 | Hit@1=0.727\n",
      "=== Hangman Finetune 86/100 — VAL   ===\n",
      "[HMG 86/100] loss 0.6078/0.5642 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 87/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2555 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2304 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2207 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2676 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2979 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3877 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4109 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4747 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5562 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5775 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6146 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6481 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6533 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6743 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6700 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6486 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6241 | Hit@1=0.727\n",
      "=== Hangman Finetune 87/100 — VAL   ===\n",
      "[HMG 87/100] loss 0.6077/0.5637 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 88/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2558 | Hit@1=0.879\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2307 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2210 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2674 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2982 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3878 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4107 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4747 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5561 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5773 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6144 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6481 | Hit@1=0.718\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6532 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6742 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6699 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6485 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6240 | Hit@1=0.727\n",
      "=== Hangman Finetune 88/100 — VAL   ===\n",
      "[HMG 88/100] loss 0.6076/0.5641 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 89/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2562 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2304 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2209 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2671 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2975 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3869 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4102 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4743 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5557 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5769 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6140 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6477 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6531 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6739 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6696 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6482 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6237 | Hit@1=0.727\n",
      "=== Hangman Finetune 89/100 — VAL   ===\n",
      "[HMG 89/100] loss 0.6073/0.5649 | Hit@1 0.734/0.757\n",
      "\n",
      "=== Hangman Finetune 90/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2565 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2299 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2208 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2672 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2974 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3872 | Hit@1=0.827\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4103 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4742 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5555 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5768 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6140 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6477 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6529 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6737 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6694 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6480 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6236 | Hit@1=0.727\n",
      "=== Hangman Finetune 90/100 — VAL   ===\n",
      "[HMG 90/100] loss 0.6072/0.5646 | Hit@1 0.734/0.756\n",
      "\n",
      "=== Hangman Finetune 91/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2562 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2300 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2207 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2671 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2974 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3869 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4100 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4741 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5557 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5770 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6140 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6476 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6528 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6737 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6695 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6480 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6235 | Hit@1=0.728\n",
      "=== Hangman Finetune 91/100 — VAL   ===\n",
      "[HMG 91/100] loss 0.6071/0.5653 | Hit@1 0.735/0.756\n",
      "\n",
      "=== Hangman Finetune 92/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2561 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2302 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2209 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2671 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2973 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3873 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4104 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4742 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5557 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5769 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6140 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6476 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6530 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6738 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6694 | Hit@1=0.708\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6481 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6236 | Hit@1=0.728\n",
      "=== Hangman Finetune 92/100 — VAL   ===\n",
      "[HMG 92/100] loss 0.6073/0.5645 | Hit@1 0.734/0.756\n",
      "\n",
      "=== Hangman Finetune 93/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2559 | Hit@1=0.879\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2303 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2210 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2673 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2977 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3874 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4101 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4740 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5553 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5765 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6135 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6471 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6525 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6734 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6691 | Hit@1=0.709\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6477 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6233 | Hit@1=0.728\n",
      "=== Hangman Finetune 93/100 — VAL   ===\n",
      "[HMG 93/100] loss 0.6069/0.5643 | Hit@1 0.735/0.757\n",
      "\n",
      "=== Hangman Finetune 94/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2559 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2296 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2205 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2667 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2973 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3869 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4100 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4740 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5553 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5764 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6136 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6472 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6524 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6734 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6691 | Hit@1=0.709\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6476 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6232 | Hit@1=0.728\n",
      "=== Hangman Finetune 94/100 — VAL   ===\n",
      "[HMG 94/100] loss 0.6068/0.5642 | Hit@1 0.735/0.757\n",
      "\n",
      "=== Hangman Finetune 95/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2563 | Hit@1=0.878\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2295 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2201 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2666 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2968 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3865 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4096 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4738 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5550 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5763 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6133 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6470 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6522 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6731 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6688 | Hit@1=0.709\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6474 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6229 | Hit@1=0.728\n",
      "=== Hangman Finetune 95/100 — VAL   ===\n",
      "[HMG 95/100] loss 0.6066/0.5642 | Hit@1 0.735/0.757\n",
      "\n",
      "=== Hangman Finetune 96/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2567 | Hit@1=0.877\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2301 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2206 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2669 | Hit@1=0.874\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2973 | Hit@1=0.861\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3870 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4099 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4736 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5550 | Hit@1=0.756\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5762 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6133 | Hit@1=0.732\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6468 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6523 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6731 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6689 | Hit@1=0.709\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6475 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6230 | Hit@1=0.728\n",
      "=== Hangman Finetune 96/100 — VAL   ===\n",
      "[HMG 96/100] loss 0.6067/0.5644 | Hit@1 0.735/0.757\n",
      "\n",
      "=== Hangman Finetune 97/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2553 | Hit@1=0.879\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2294 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2201 | Hit@1=0.895\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2664 | Hit@1=0.875\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2969 | Hit@1=0.862\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3865 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4095 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4735 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5547 | Hit@1=0.757\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5760 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6132 | Hit@1=0.733\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6466 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6520 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6731 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6686 | Hit@1=0.709\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6472 | Hit@1=0.718\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6227 | Hit@1=0.728\n",
      "=== Hangman Finetune 97/100 — VAL   ===\n",
      "[HMG 97/100] loss 0.6063/0.5647 | Hit@1 0.735/0.757\n",
      "\n",
      "=== Hangman Finetune 98/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2551 | Hit@1=0.879\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2297 | Hit@1=0.891\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2204 | Hit@1=0.895\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2667 | Hit@1=0.875\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2968 | Hit@1=0.862\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3862 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4094 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4730 | Hit@1=0.791\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5545 | Hit@1=0.757\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5757 | Hit@1=0.748\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6130 | Hit@1=0.733\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6466 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6519 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6728 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6685 | Hit@1=0.709\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6470 | Hit@1=0.718\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6226 | Hit@1=0.728\n",
      "=== Hangman Finetune 98/100 — VAL   ===\n",
      "[HMG 98/100] loss 0.6062/0.5648 | Hit@1 0.735/0.757\n",
      "\n",
      "=== Hangman Finetune 99/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2546 | Hit@1=0.879\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2294 | Hit@1=0.890\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2201 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2660 | Hit@1=0.875\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2964 | Hit@1=0.862\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3861 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4092 | Hit@1=0.817\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4731 | Hit@1=0.790\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5544 | Hit@1=0.757\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5759 | Hit@1=0.747\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6129 | Hit@1=0.733\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6465 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6517 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6728 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6685 | Hit@1=0.709\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6472 | Hit@1=0.717\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6227 | Hit@1=0.728\n",
      "=== Hangman Finetune 99/100 — VAL   ===\n",
      "[HMG 99/100] loss 0.6063/0.5650 | Hit@1 0.735/0.757\n",
      "\n",
      "=== Hangman Finetune 100/100 — TRAIN ===\n",
      "  [train] step   200 | seen   409600/7167888 | len=12 | batch=2048 | loss=0.2543 | Hit@1=0.880\n",
      "  [train] step   400 | seen   817212/7167888 | len=13 | batch=2048 | loss=0.2294 | Hit@1=0.891\n",
      "  [train] step   600 | seen  1226812/7167888 | len=13 | batch=2048 | loss=0.2201 | Hit@1=0.894\n",
      "  [train] step   800 | seen  1633136/7167888 | len=10 | batch=2048 | loss=0.2661 | Hit@1=0.875\n",
      "  [train] step  1000 | seen  2042736/7167888 | len=10 | batch=2048 | loss=0.2964 | Hit@1=0.862\n",
      "  [train] step  1200 | seen  2448724/7167888 | len=9  | batch=2048 | loss=0.3858 | Hit@1=0.828\n",
      "  [train] step  1400 | seen  2858324/7167888 | len=9  | batch=2048 | loss=0.4092 | Hit@1=0.818\n",
      "  [train] step  1600 | seen  3267896/7167888 | len=5  | batch=2048 | loss=0.4731 | Hit@1=0.791\n",
      "  [train] step  1800 | seen  3675784/7167888 | len=17 | batch=2048 | loss=0.5543 | Hit@1=0.757\n",
      "  [train] step  2000 | seen  4083800/7167888 | len=7  | batch=2048 | loss=0.5755 | Hit@1=0.748\n",
      "  [train] step  2200 | seen  4488836/7167888 | len=4  | batch=2048 | loss=0.6125 | Hit@1=0.733\n",
      "  [train] step  2400 | seen  4897428/7167888 | len=8  | batch=2048 | loss=0.6463 | Hit@1=0.719\n",
      "  [train] step  2600 | seen  5307028/7167888 | len=8  | batch=2048 | loss=0.6516 | Hit@1=0.716\n",
      "  [train] step  2800 | seen  5704792/7167888 | len=6  | batch=2048 | loss=0.6725 | Hit@1=0.707\n",
      "  [train] step  3000 | seen  6112964/7167888 | len=11 | batch=2048 | loss=0.6683 | Hit@1=0.709\n",
      "  [train] step  3200 | seen  6522564/7167888 | len=11 | batch=2048 | loss=0.6469 | Hit@1=0.718\n",
      "  [train] step  3400 | seen  6929680/7167888 | len=14 | batch=2048 | loss=0.6224 | Hit@1=0.728\n",
      "=== Hangman Finetune 100/100 — VAL   ===\n",
      "[HMG 100/100] loss 0.6061/0.5643 | Hit@1 0.735/0.757\n",
      "Saved fine-tuned model → m11.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def finetune_hangman(model, train_states, val_states, epochs=FT_EPOCHS,\n",
    "                     batch_size=FT_BATCH, lr=FT_LR, log_every=200):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    def run_epoch(by_len, train=True):\n",
    "        model.train(train)\n",
    "        total, loss_sum, hit1_sum, steps = 0, 0.0, 0.0, 0\n",
    "        epoch_total = sum(len(v) for v in by_len.values())\n",
    "        gen = hm_batches_from_states(by_len, batch_size=batch_size, shuffle=True)\n",
    "        for xs, mask, L in gen:\n",
    "            xs   = xs.to(DEVICE)\n",
    "            mask = mask.to(DEVICE)\n",
    "            with torch.set_grad_enabled(train):\n",
    "                logits = model.forward_cls(xs)           # [B,26]\n",
    "                p = torch.softmax(logits, dim=1)         # [B,26]\n",
    "                mass = (p * mask).sum(dim=1).clamp_min(1e-8)\n",
    "                loss = -torch.log(mass).mean()\n",
    "                if train:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    opt.step()\n",
    "            with torch.no_grad():\n",
    "                pred1 = logits.argmax(dim=1)\n",
    "                hit = mask[torch.arange(mask.size(0), device=mask.device), pred1].gt(0).float().sum().item()\n",
    "                bsz = mask.size(0)\n",
    "                total    += bsz\n",
    "                loss_sum += loss.item() * bsz\n",
    "                hit1_sum += hit\n",
    "                steps += 1\n",
    "                if log_every and (steps % log_every == 0):\n",
    "                    print(f\"  [{'train' if train else 'val'}] step {steps:>5} | seen {total:>8}/{epoch_total} \"\n",
    "                          f\"| len={L:<2} | batch={bsz:<4} | loss={loss_sum/max(total,1):.4f} | Hit@1={hit1_sum/max(total,1):.3f}\")\n",
    "        return loss_sum/max(total,1), hit1_sum/max(total,1)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        print(f\"\\n=== Hangman Finetune {ep}/{epochs} — TRAIN ===\")\n",
    "        tr_loss, tr_hit1 = run_epoch(train_states, train=True)\n",
    "        print(f\"=== Hangman Finetune {ep}/{epochs} — VAL   ===\")\n",
    "        va_loss, va_hit1 = run_epoch(val_states,   train=False)\n",
    "        print(f\"[HMG {ep}/{epochs}] loss {tr_loss:.4f}/{va_loss:.4f} | Hit@1 {tr_hit1:.3f}/{va_hit1:.3f}\")\n",
    "\n",
    "    ckpt = {\n",
    "        \"config\": {\"vocab_size\": VOCAB_SIZE, \"d_model\": DMODEL, \"n_heads\": N_HEADS,\n",
    "                   \"n_layers\": N_LAYERS, \"d_ff\": D_FF, \"max_len\": MAX_LEN},\n",
    "        \"state_dict\": model.state_dict(),\n",
    "    }\n",
    "    torch.save(ckpt, \"m11.pt\")\n",
    "    print(\"Saved fine-tuned model → m11.pt\")\n",
    "    return model\n",
    "\n",
    "# Run finetune\n",
    "model = finetune_hangman(model, train_states, val_states, epochs=FT_EPOCHS, batch_size=FT_BATCH, lr=FT_LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ec30e-0185-4164-a7ad-b485f74406dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
